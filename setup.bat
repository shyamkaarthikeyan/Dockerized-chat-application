@echo off
REM Real-time Chat Application Setup Script for Windows
REM This script automates the deployment and initial setup

echo ğŸš€ Starting Real-time Chat Application Setup...

REM Check if Docker is installed
docker --version >nul 2>&1
if errorlevel 1 (
    echo âŒ Docker is not installed. Please install Docker Desktop first.
    pause
    exit /b 1
)

REM Check if Docker Compose is installed
docker-compose --version >nul 2>&1
if errorlevel 1 (
    echo âŒ Docker Compose is not installed. Please install Docker Compose first.
    pause
    exit /b 1
)

echo âœ… Docker and Docker Compose are available

REM Start the services
echo ğŸ“¦ Starting all services with Docker Compose...
docker-compose up -d

echo â³ Waiting for services to start...
timeout /t 10 /nobreak >nul

REM Wait for Ollama to be ready
echo ğŸ¤– Waiting for Ollama to be ready...
set max_attempts=30
set attempt=0

:wait_loop
set /a attempt+=1
curl -f http://localhost:11434/api/tags >nul 2>&1
if not errorlevel 1 (
    echo âœ… Ollama is ready!
    goto ollama_ready
)

if %attempt% geq %max_attempts% (
    echo âŒ Ollama failed to start within expected time
    echo ğŸ“‹ Check logs with: docker-compose logs ollama
    pause
    exit /b 1
)

echo â³ Attempt %attempt%/%max_attempts% - Ollama not ready yet...
timeout /t 10 /nobreak >nul
goto wait_loop

:ollama_ready
REM Pull the default LLM model
echo ğŸ“¥ Pulling default LLM model (llama3.2)...
docker exec ollama ollama pull llama3.2

REM Optional: Pull additional models
set /p choice="ğŸ¤” Do you want to pull additional models? (y/n): "
if /i "%choice%"=="y" (
    echo ğŸ“¥ Pulling additional models...
    docker exec ollama ollama pull mistral
    docker exec ollama ollama pull neural-chat
    echo âœ… Additional models downloaded!
)

REM Check if all services are healthy
echo ğŸ” Checking service health...

REM Check backend
curl -f http://localhost:3001/health >nul 2>&1
if not errorlevel 1 (
    echo âœ… Backend is healthy
) else (
    echo âš ï¸ Backend health check failed
)

REM Check frontend
curl -f http://localhost:8501/_stcore/health >nul 2>&1
if not errorlevel 1 (
    echo âœ… Frontend is healthy
) else (
    echo âš ï¸ Frontend health check failed
)

echo.
echo ğŸ‰ Setup complete!
echo.
echo ğŸ“‹ Access your application:
echo    Frontend: http://localhost:8501
echo    Backend Health: http://localhost:3001/health
echo    Ollama API: http://localhost:11434/api/tags
echo.
echo ğŸ“– Useful commands:
echo    View logs: docker-compose logs -f
echo    Stop services: docker-compose down
echo    Restart: docker-compose restart
echo.
echo ğŸš€ Happy chatting with your local LLM!
pause